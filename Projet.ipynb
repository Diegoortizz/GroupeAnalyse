{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    passwd=\"admin\",\n",
    "    database=\"granddebattest\"\n",
    ")\n",
    "\n",
    "cursor = db.cursor(dictionary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT th.nom_Theme, arg.texte, arg.type \n",
    "FROM `contribution` AS contr JOIN `arguments` AS arg USING(id_contrib) JOIN `theme` AS th ON contr.FK_theme_id = th.id_theme\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arguments = pd.DataFrame(arguments, columns=arguments[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments.to_csv(\"data/argumentsVD.csv\", index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandarallel import pandarallel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_GDN = [\"31-01-19\",\"06-02-19\",\"17-02-19\",\"02-03-19\",\"08-03-19\",\"21-03-19\"]\n",
    "sujets_GDN = {\"ecologie\": \"LA_TRANSITION_ECOLOGIQUE\",\n",
    "             \"democratie\": \"DEMOCRATIE_ET_CITOYENNETE\",\n",
    "             \"fisc\": \"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES\",\n",
    "             \"etat\": \"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS\"}\n",
    "sujets_VD = os.listdir(\"data/VD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Plus efficace\n",
    "flatten = lambda l: list(itertools.chain(*l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_flatten_tags = lambda docs: [t for tokens in docs for t in tokens if not (t.is_punct or t.is_space or t.is_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"data/GDN/{sujets_GDN['etat']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = nlp(df[df.columns[-1]].dropna().values[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = pd.read_csv(\"data/GDN/CR_RIL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_colonnes_utiles = [3,4,5,6,9,10,12,13,22,23,24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set(resume[resume.columns[-1]].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume[resume[resume.columns[-1]] != \"Le document envoyé n'est pas un compte rendu \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume[resume.Ville == \"ARGENTEUIL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resume.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "core_resume = resume[resume.columns[id_colonnes_utiles]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_resume.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = core_resume[core_resume.columns[6]].dropna().str.lower().str.split(\" \", n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = core_resume[((core_resume[core_resume.columns[6]].str.lower().str.split(\" \", n=1).str[0] != \"cf\") | (core_resume[core_resume.columns[6]].str.lower().str.split(\" \", n=1).str[0] != \"voir\")) & (core_resume[core_resume.columns[6]].str.len() < 10)]#.str.lower().str.split(\" \", n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(test[test.columns[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [mot for mot in test if not (mot.startswith(\"cf\") or mot.startswith(\"voir\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_constats = list(nlp.pipe(core_resume[core_resume.columns[6]].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_props = list(nlp.pipe(core_resume[core_resume.columns[7]].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_constats[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_constats = [(token.lemma_, token.pos_) for doc in all_constats for token in doc if not (token.is_punct or token.is_space or token.is_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_props = [(token.lemma_, token.pos_) for doc in all_props for token in doc if not (token.is_punct or token.is_space or token.is_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constats_lemmas = [lemma for lemma, _ in filtered_constats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_lemmas = [lemma for lemma, _ in filtered_props]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_ngrams(data, n, nb):\n",
    "    n_grams = ngrams(data, n)\n",
    "    print(Counter(n_grams).most_common(nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_first_ngrams(constats_lemmas, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(filtered_props).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sorted(resume[resume.columns[0]]))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in t.doc:\n",
    "    if not (token.is_punct or token.is_space or token.is_stop):\n",
    "        print(token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(27086, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = df.groupby(\"region\")\n",
    "author = df.groupby(\"authorType\")\n",
    "\n",
    "region_test = reg.get_group('Corse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for group in author.groups:\n",
    "    print(group, author.get_group(group).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_group(list(reg.groups.keys())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(author.get_group(list(author.groups.keys())[1]).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_min_rows = lambda groups: min([groups.get_group(group).shape[0] for group in groups.groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampled_df(groups):\n",
    "    sampled_dfs = []\n",
    "\n",
    "    min_group = get_min_rows(groups)\n",
    "\n",
    "\n",
    "    for group in groups.groups:\n",
    "        group_data = groups.get_group(group)\n",
    "        sampled_dfs.append(group_data.loc[np.random.RandomState(seed=seed).permutation(group_data.index)[:min(group_data.shape[0], 1000)]])\n",
    "\n",
    "    sampled_df = pd.concat(sampled_dfs)\n",
    "    return sampled_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([reg, author], axi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reg = get_sampled_df(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sampled_reg[[sampled_reg.columns[-1]]].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['tokens'] = sample[sample.columns[0]].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sampled_df(reg).to_csv(\"data/sampled_regions.csv\", index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sampled_df(author).to_csv(\"data/sampled_authors.csv\", index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv(\"data/sampled_regions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(\"data/sampled_authors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corse = reg.get_group('Corse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_print = df[[\"region\"]].groupby(\"region\")\n",
    "\n",
    "for name, group in reg_print:\n",
    "    print(name)\n",
    "    print(group.shape)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_question_fermee(df,column, debug=False):\n",
    "    nb_answers = len(df[column].unique())\n",
    "    if debug:\n",
    "        print(column)\n",
    "        print(\"-\"*20)\n",
    "        print(f\"Taille: {nb_answers} ==> {'FERMEE' if nb_answers < 20 else 'OUVERTE'}\")\n",
    "        print(\"=\"*50)\n",
    "    return nb_answers < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_indices(df):\n",
    "    return df.drop(df.columns[:6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions_fermees(df):\n",
    "    return df.drop(df.columns[[i for i,column in enumerate(df.columns) if is_question_fermee(df,column) or i<6]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df(df):\n",
    "    #indexes = ['docs', 'nb_prop', 'question_freq', 'tokens', 'pos', 'propLen_moy', 'nb_phrases_moy', 'unigram', 'bigram']\n",
    "    indexes = ['nb_prop', 'question_freq', 'propLen_moy', 'nb_phrases_moy', 'unigram', 'bigram']\n",
    "    \n",
    "    questions_only = remove_first_indices(df)\n",
    "    #ouvertes_only = remove_questions_fermees(df)\n",
    "\n",
    "    new_df = pd.DataFrame(index=questions_only.columns, columns=indexes)\n",
    "    \n",
    "    return new_df, questions_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set().union([\"abc\", \"def\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = StanfordPOSTagger(\"stanford/models/french.tagger\", \"stanford/stanford-postagger.jar\", encoding='utf8') #instance de la classe StanfordPOSTagger en UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stops = set(stopwords.words('french')).union(string.punctuation+\"’\").union([\"...\", \"--\"])\n",
    "\n",
    "def fill_df(df, data):\n",
    "    #Listes pour pouvoir les insérer dans la dataframe\n",
    "    for i,index in enumerate(df.index):\n",
    "        df.loc[index]['nb_prop'] = data[index].dropna().shape[0]  \n",
    "        df.loc[index]['question_freq'] = round(df.loc[index]['nb_prop']/data[index].shape[0], 2)\n",
    "        #df.loc[index]['docs'] = list(nlp.pipe(data[index].dropna()))\n",
    "        #df.loc[index]['tokens'] = filter_flatten_tags(df.loc[index]['docs'])\n",
    "        #df.loc[index]['pos'] = [(token.lemma_, token.pos_) for token in df.loc[index]['tokens']]\n",
    "        \n",
    "        if not is_question_fermee(data, index):\n",
    "            #df.loc[index]['propLen_moy'] = sum([len(doc) for doc in df.loc[index]['docs']])/df.loc[index]['nb_prop']\n",
    "            df.loc[index]['propLen_moy'] = sum(data[data.columns[i]].dropna().apply(lambda prop: len(prop.split())))/df.loc[index]['nb_prop']\n",
    "            #df.loc[index]['nb_phrases_moy'] = sum([len(TextBlob(doc.text).sentences) for doc in df.loc[index]['docs']])/df.loc[index]['nb_prop']\n",
    "            df.loc[index]['nb_phrases_moy'] = sum([len(TextBlob(prop).sentences) for prop in data[data.columns[i]].dropna()])/df.loc[index]['nb_prop']\n",
    "\n",
    "            lemmas = flatten([nltk.word_tokenize(prop.lower()) for prop in data[index].dropna().values])\n",
    "            lemmas = [lemma for lemma in lemmas if lemma not in stops]\n",
    "            #if(i==1):\n",
    "                #print(sorted(lemmas))\n",
    "            df.loc[index]['unigram'] = Counter(ngrams(lemmas, 1)).most_common(5)\n",
    "            df.loc[index]['bigram'] = Counter(ngrams(lemmas, 2)).most_common(5)\n",
    "        else:\n",
    "            \n",
    "            df.loc[index]['propLen_moy'] = \"question fermée\"\n",
    "            df.loc[index]['nb_phrases_moy'] = \"question fermée\"\n",
    "            \n",
    "            df.loc[index]['unigram'] = \"question fermée\"\n",
    "            df.loc[index]['bigram'] = \"question fermée\"\n",
    "            \n",
    "        #print(f\"Question {i+1}/{len(df.index)} done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def fill_df(df, data):\n",
    "    nb_prop = []\n",
    "    docs = []\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    propLen_moy = []\n",
    "    nb_phrases_moy = []\n",
    "    unigram = []\n",
    "    bigram = []\n",
    "\n",
    "    #Listes pour pouvoir les insérer dans la dataframe\n",
    "    for col in data.columns:\n",
    "        nb_prop.append(data[col].shape[0])\n",
    "        docs.append(list(nlp.pipe(data[col].dropna())))\n",
    "        #docs -> tokens\n",
    "        tokens.append(filter_flatten_tags(docs[-1]))\n",
    "        #tokens -> pos\n",
    "        pos.append([(token.lemma_, token.pos_) for token in tokens[-1]])\n",
    "        #docs & nb_prop -> propLen_moy\n",
    "        propLen_moy.append(sum([len(doc) for doc in docs[-1]])/nb_prop[-1])\n",
    "        #docs & nb_prop -> nb_phrases_moy\n",
    "        nb_phrases_moy.append(sum([len(TextBlob(doc.text).sentences) for doc in docs[-1]])/nb_prop[-1])\n",
    "        #pos -> unigram\n",
    "        unigram.append(list(ngrams([lemma for lemma, _ in pos[-1]], 1)))\n",
    "        #pos -> bigram\n",
    "        bigram.append(list(ngrams([lemma for lemma, _ in pos[-1]], 2)))\n",
    "        \n",
    "        print(f\"Col {len(nb_prop)}/{len(data.columns)} done!\")\n",
    "\n",
    "    #Nombre de propositions\n",
    "    df.loc['nb_prop'] = nb_prop\n",
    "\n",
    "    #Chaque proposition sous forme de Doc spacy\n",
    "    df.loc['docs'] = docs\n",
    "\n",
    "    #Liste de tokens de toutes les propositions combinées\n",
    "    df.loc['tokens'] = tokens\n",
    "\n",
    "    #POS tag + lemme pour chaque token\n",
    "    df.loc['POS'] = pos\n",
    "\n",
    "    #Longueur moyenne d'une proposition\n",
    "    df.loc['propLen_moy'] = propLen_moy\n",
    "\n",
    "    #Nombre de phrases moyen pour une proposition\n",
    "    df.loc['nb_phrases_moy'] = nb_phrases_moy\n",
    "\n",
    "    #Unigram sur les tokens\n",
    "    df.loc['unigram'] = unigram\n",
    "\n",
    "    #Bigram sur les tokens\n",
    "    df.loc['bigram'] =  bigram\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(df):\n",
    "    new_df, data = init_df(df)\n",
    "    fill_df(new_df, data)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions_dfs(df):\n",
    "    dfs = dict()\n",
    "    \n",
    "    reg = df.groupby(\"region\")\n",
    "    for i,region in enumerate(reg.groups):\n",
    "        if region not in {\"Normandie\", \"Corse\"}:\n",
    "            new_df = get_df(reg.get_group(region))\n",
    "            new_df.to_csv(f\"data/GDN_saves/{region}_INV.csv\", header=True)\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            print(f\"({i+1}/{len(list(reg.groups))}) {region} TERMINEE\", end=\" \")\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, corse = init_df(region_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corse[corse.columns[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"\"\"\n",
    "import nltk\n",
    "from __main__ import corse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = calc_all_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(save, open(\"data/save.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/GDN_saves/Corse_INV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['docs'].apply(lambda x: type(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pickle.load(open(\"data/save.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['docs'].apply(lambda x: type(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_regions_dfs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_docs(row):\n",
    "    return nlp.pipe(data[row.name].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"problematique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_row(row):\n",
    "    row.nb_prop = data[row.name].shape[0]  \n",
    "    row.tokens = filter_flatten_tags(row.docs)\n",
    "    row.pos = [(token.lemma_, token.pos_) for token in row.tokens]\n",
    "    row.propLen_moy = sum([len(doc) for doc in row.docs])/row.nb_prop\n",
    "    row.nb_phrases_moy = sum([len(TextBlob(doc.text).sentences) for doc in row.docs])/row.nb_prop\n",
    "    row.unigram = list(ngrams([lemma for lemma, _ in row.pos], 1))\n",
    "    row.bigram = list(ngrams([lemma for lemma, _ in row.pos], 2))\n",
    "        \n",
    "    return row\n",
    "    #print(f\"Question {i+1}/{len(df.index)} done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_row(row):\n",
    "    #Listes pour pouvoir les insérer dans la dataframe\n",
    "    row.nb_prop = data[row.name].dropna().shape[0]  \n",
    "    row.question_freq = round(row.nb_prop/data[row.name].shape[0], 2)\n",
    "    #df.loc[index]['docs'] = list(nlp.pipe(data[index].dropna()))\n",
    "    #df.loc[index]['tokens'] = filter_flatten_tags(df.loc[index]['docs'])\n",
    "    #df.loc[index]['pos'] = [(token.lemma_, token.pos_) for token in df.loc[index]['tokens']]\n",
    "\n",
    "    if not is_question_fermee(data, row.name):\n",
    "        #df.loc[index]['propLen_moy'] = sum([len(doc) for doc in df.loc[index]['docs']])/df.loc[index]['nb_prop']\n",
    "        row.propLen_moy = sum(data[row.name].dropna().apply(lambda prop: len(prop.split())))/row.nb_prop\n",
    "        #df.loc[index]['nb_phrases_moy'] = sum([len(TextBlob(doc.text).sentences) for doc in df.loc[index]['docs']])/df.loc[index]['nb_prop']\n",
    "        row.nb_phrases_moy = sum([len(TextBlob(prop).sentences) for prop in data[row.name].dropna()])/row.nb_prop\n",
    "        \n",
    "        #docs = filter_flatten_tags(nlp.pipe(data[row.name].dropna()))\n",
    "        \n",
    "        #lemmas = [token.lemma_ for token in docs]\n",
    "        \n",
    "        stems = flatten([nltk.word_tokenize(prop.lower(), language=\"french\") for prop in data[row.name].dropna().values])\n",
    "        stems = [stemmer.stem(w) for w in stems if w not in stops]\n",
    "        #lemmas = flatten([nltk.pos_tag(nltk.word_tokenize(prop.lower(), language=\"french\")) for prop in data[row.name].dropna().values])\n",
    "        #lemmas = [word for (word, tag) in lemmas if tag not in set(\".:()\")]\n",
    "        #if(i==1):\n",
    "            #print(sorted(lemmas))\n",
    "        row.unigram = Counter(ngrams(stems, 1)).most_common(10)\n",
    "        row.bigram = Counter(ngrams(stems, 2)).most_common(10)\n",
    "    else:\n",
    "        row.propLen_moy = \"question fermée\"\n",
    "        row.nb_phrases_moy = \"question fermée\"\n",
    "\n",
    "        row.unigram = \"question fermée\"\n",
    "        row.bigram = \"question fermée\"\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corse = get_df(region_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2, data = init_df(region_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"en mon humble opinion personellement je trouve ça plutôt banale tout ça\"\n",
    "blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[1][\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary, data = init_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "full_summary = full_summary.parallel_apply(fill_row, axis=1)\n",
    "print(f\"Time: {time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region, group in reg.groups:\n",
    "    print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in reg.groups:\n",
    "    print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[-1]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in reg.groups:\n",
    "    summary, data = init_df(reg.get_group(region))\n",
    "    start = time()\n",
    "    summary = summary.parallel_apply(fill_row, axis=1)\n",
    "    summary.to_csv(f\"data/eco{region}Summary.csv\")\n",
    "    print(f\"Time: {time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "test2 = test2.apply(fill_row, axis=1)\n",
    "print(f\"Time: {time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary.to_csv(\"data/ecoFullSummary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Nombre de propositions\n",
    "test.loc['nb_prop'] = ouvertes_only[ouvertes_only.columns[1]].shape[0]\n",
    "\n",
    "#Chaque proposition sous forme de Doc spacy\n",
    "test.loc['docs'] = [list(nlp.pipe(ouvertes_only[ouvertes_only.columns[1]].dropna()))]\n",
    "\n",
    "#Liste de tokens de toutes les propositions combinées\n",
    "test.loc['tokens'] = [filter_flatten_tags(test[test.columns[0]]['docs'])]\n",
    "\n",
    "#POS tag + lemme pour chaque token\n",
    "test.loc['POS'] = [[(token.lemma_, token.pos_) for token in test[test.columns[0]]['tokens']]]\n",
    "\n",
    "#Longueur moyenne d'une proposition\n",
    "test.loc['propLen_moy'] = sum([len(doc) for doc in test[test.columns[0]]['docs']])/test[test.columns[0]]['nb_prop']\n",
    "\n",
    "#Nombre de phrases moyen pour une proposition\n",
    "test.loc['nb_phrases_moy'] = sum([len(TextBlob(doc.text).sentences) for doc in test[test.columns[0]]['docs']])/test[test.columns[0]]['nb_prop']\n",
    "\n",
    "#Unigram sur les tokens\n",
    "test.loc['unigram'] = [list(ngrams([lemma for lemma, _ in test[test.columns[0]]['POS']], 1))]\n",
    "\n",
    "#Bigram sur les tokens\n",
    "test.loc['bigram'] =  [list(ngrams([lemma for lemma, _ in test[test.columns[0]]['POS']], 2))]\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter(test[test.columns[0]]['unigram'])\n",
    "bigrams =  Counter(test[test.columns[0]]['bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "docs = list(nlp.pipe(ouvertes_only[ouvertes_only.columns[1]].dropna()))\n",
    "print(f\"Time: {time()-start:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
