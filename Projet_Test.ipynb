{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandarallel import pandarallel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_GDN = [\"31-01-19\",\"06-02-19\",\"17-02-19\",\"02-03-19\",\"08-03-19\",\"21-03-19\"]\n",
    "sujets_GDN = {\"ecologie\": \"LA_TRANSITION_ECOLOGIQUE\",\n",
    "             \"democratie\": \"DEMOCRATIE_ET_CITOYENNETE\",\n",
    "             \"fisc\": \"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES\",\n",
    "             \"etat\": \"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS\"}\n",
    "sujets_VD = os.listdir(\"data/VD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Plus efficace\n",
    "flatten = lambda l: list(itertools.chain(*l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_flatten_tags = lambda docs: [t for tokens in docs for t in tokens if not (t.is_punct or t.is_space or t.is_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichier utilisé actuellement pour les tests:\n",
    "\n",
    "`2019-03-04_justice-police-armee_consultation-3.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"data/VD/2019-03-04_justice-police-armee_consultation-3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = df[[\"contributions_bodyText\"]].dropna()\n",
    "body.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 1400 Propositions au total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "docs = list(nlp.pipe(body[\"contributions_bodyText\"]))\n",
    "print([list(filter(lambda t: t.pos_ not in {\"SPACE\", \"PUNCT\"}, tokens)) for tokens in docs])\n",
    "print(f\"Time: {time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t for tokens in docs for t in tokens if t.pos_ not in {\"SPACE\",\"PUNCT\"} ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('flatten([list(filter(lambda t: t.pos_ not in {\"SPACE\", \"PUNCT\"}, tokens)) for tokens in docs])', \"from __main__ import docs, flatten\", number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('[t for tokens in docs for t in tokens if t.pos_ not in {\"SPACE\",\"PUNCT\"} ]', \"from __main__ import docs, flatten\", number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeit.timeit('[t for tokens in docs for t in tokens if not (t.is_punct or t.is_space or t.is_stop)]', \"from __main__ import docs, flatten\", number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timeit.timeit('[t for tokens in docs for t in tokens if not t.is_punct and not t.is_space and not t.is_stop]', \"from __main__ import docs, flatten\", number=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compréhesion de liste plus rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les propositions tokenisées\n",
    "start = time()\n",
    "body[\"tokens\"] = body[\"contributions_bodyText\"].apply(lambda doc: [mot for mot in nlp(doc) if not (mot.is_stop or mot.is_punct or len(mot) <3)])\n",
    "print(f\"Time: {time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemme de chaque token\n",
    "body[\"lemmas\"] = body[\"tokens\"].apply(lambda tokens: [token.lemma_ for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le part-of-speech tag pour chaque token, avec son lemme\n",
    "body[\"tags\"] = body[\"tokens\"].apply(lambda tokens: [(token.pos_, token.lemma_) for token in tokens])#(\"PUNCT\", \"SPACE\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre de tokens par proposition\n",
    "body[\"propLen\"] = body[\"tokens\"].apply(lambda l: len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre de phrases par proposition\n",
    "body[\"nbPhrases\"] = body[\"contributions_bodyText\"].apply(lambda doc: len(TextBlob(doc).sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "body[\"unigram\"] = body[\"lemmas\"].apply(lambda tokens: list(ngrams(tokens,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram\n",
    "body[\"bigram\"] = body[\"lemmas\"].apply(lambda tokens: list(ngrams(tokens,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(body['lemmas'][0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = flatten(body[\"unigram\"].tolist())\n",
    "bigrams = flatten(body[\"bigram\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramCounter = Counter(bigrams)\n",
    "unigramCounter = Counter(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramCounter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramCounter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_dict(posList):\n",
    "    pos_dict = dict()\n",
    "    for lemma, tag in posList:\n",
    "        pos_dict.setdefault(tag, []).append(lemma)\n",
    "    return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_counter_dict(pos_data):\n",
    "    pos_counters = dict()\n",
    "    for pos in pos_data:\n",
    "        pos_counters[pos] = Counter(pos_data[pos])\n",
    "    return pos_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos_data = create_pos_dict(flatten(body[\"tags\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counters = dict()\n",
    "for pos in pos_data:\n",
    "    pos_counters[pos] = Counter(pos_data[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsp_most_common_pos(dict_data, dict_counters, n):\n",
    "    for tag in dict_data:\n",
    "        print(f'Les {n} {spacy.explain(tag)} les plus fréquents :')\n",
    "        for mot in dict_counters[tag].most_common(n):\n",
    "            print(mot)\n",
    "        print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsp_most_common_pos(pos_data, pos_counters, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
